{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def masterfile():\n",
    "    # Paths\n",
    "    folderPath = \"FOLDER\"  # Update this\n",
    "    masterPath = \"Master datasheet.xlsx\"  # Update this\n",
    "    outputPath = \"master/master.xlsx\"  # Update this\n",
    "\n",
    "    def generateCol(n):\n",
    "        \"\"\" Generate Excel-style column names (A, B, C... AA, AB, etc.). \"\"\"\n",
    "        names = []\n",
    "        for i in range(n):\n",
    "            col_name = ''\n",
    "            while i >= 0:\n",
    "                col_name = chr(i % 26 + 65) + col_name\n",
    "                i = i // 26 - 1\n",
    "            names.append(col_name)\n",
    "        return names\n",
    "\n",
    "    # Read the master template\n",
    "    master = pd.read_excel(masterPath)\n",
    "    masterColnames = master.columns  # Preserve original column names\n",
    "    n = len(master.columns)\n",
    "    Colnames = generateCol(n)\n",
    "    master.columns = Colnames  # Use temporary column names for processing\n",
    "\n",
    "    # Loop through all folders in the directory\n",
    "    for foldername in os.listdir(folderPath):\n",
    "        paths = os.path.join(folderPath, foldername)\n",
    "        if not os.path.isdir(paths):  # Skip files, only process directories\n",
    "            continue\n",
    "        \n",
    "        for filename in os.listdir(paths):\n",
    "            filePath = os.path.join(paths, filename)\n",
    "\n",
    "            try:\n",
    "                # Read the column mapping sheet (handle missing sheets)\n",
    "                try:\n",
    "                    Col_directions = pd.read_excel(masterPath, sheet_name=foldername)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Sheet '{foldername}' not found in master template. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                if not {'From', 'To'}.issubset(Col_directions.columns):\n",
    "                    print(f\"Error: Mapping sheet {foldername} must contain 'From' and 'To' columns.\")\n",
    "                    continue\n",
    "\n",
    "                # Read the file to process\n",
    "                fileRead = pd.read_excel(filePath)\n",
    "                temp_data = []  # Temporary storage for processed rows\n",
    "\n",
    "                # Breaking measurement columns into M1, M2, M3\n",
    "                if 'Measurement' in fileRead.columns:\n",
    "                    print(fileRead.columns)\n",
    "                    fileRead[['m1', 'm2', 'm3']] = fileRead['Measurement'].str.split(' x ', expand=True)\n",
    "                    fileRead[['m1', 'm2', 'm3']] = fileRead[['m1', 'm2', 'm3']].apply(pd.to_numeric, errors='coerce')\n",
    "                    # print(fileRead[['m1', 'm2', 'm3']])\n",
    "\n",
    "                def extract_dimensions(measurement):\n",
    "                    match = re.match(r\"(\\d+\\.\\d+)\\s*-\\s*(\\d+\\.\\d+)\\s*\\*\\s*(\\d+\\.\\d+)\", str(measurement))\n",
    "                    return match.groups() if match else (None, None, None)\n",
    "           \n",
    "                if 'Diameter' in fileRead.columns:\n",
    "                    fileRead[['m1', 'm2', 'm3']] = fileRead['Diameter'].apply(lambda x: pd.Series(extract_dimensions(x)))\n",
    "                    # print(fileRead[['m1', 'm2', 'm3']])\n",
    "                # Assign dynamic column names to the current file\n",
    "                n = len(fileRead.columns)\n",
    "                Colnames = generateCol(n)\n",
    "                fileRead.columns = Colnames\n",
    "\n",
    "                # Process rows based on column mapping\n",
    "                for _, row in fileRead.iterrows():\n",
    "                    # if pd.isna(row['A']):  # Stop processing if empty row\n",
    "                    #     break\n",
    "\n",
    "                    new_row = {'A': filename}  # Track file name for reference\n",
    "\n",
    "                    for _, movement_row in Col_directions.iterrows():\n",
    "                        from_col = movement_row['From']\n",
    "                        to_col = movement_row['To']\n",
    "\n",
    "                        if from_col in row.index and to_col in master.columns:\n",
    "                            new_row[to_col] = row[from_col]\n",
    "\n",
    "                    temp_data.append(new_row)  # Add processed row\n",
    "\n",
    "                # Append processed data to master\n",
    "                temp_df = pd.DataFrame(temp_data)\n",
    "                master = pd.concat([master, temp_df], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {filename}: {e}\")\n",
    "                continue\n",
    "            \n",
    "    # Function to calculate 'R' column\n",
    "    def calculate_R(row):\n",
    "        if 'P' in row and 'Q' in row and pd.notna(row['P']) and pd.notna(row['Q']):\n",
    "            return ((100 - row['Q']) / 100) * row['P'] if row['Q'] > 0 else ((100 + row['Q']) / 100) * row['P']\n",
    "        return None\n",
    "\n",
    "    # Function to calculate 'S' column\n",
    "    def calculate_S(row):\n",
    "        if 'R' in row and 'C' in row and pd.notna(row['R']) and pd.notna(row['C']):\n",
    "            return round(row['R'] * row['C'])  # Perform calculation\n",
    "        return None\n",
    "\n",
    "    # Apply calculations\n",
    "    if 'P' in master.columns and 'Q' in master.columns:\n",
    "        master['R'] = master.apply(calculate_R, axis=1)\n",
    "    if 'R' in master.columns and 'C' in master.columns:\n",
    "        master['S'] = master.apply(calculate_S, axis=1)\n",
    "\n",
    "    # Clean column 'I'\n",
    "    if 'I' in master.columns:\n",
    "        master['I'] = master['I'].apply(lambda x: None if pd.isna(x) or str(x).strip() == '' else x)\n",
    "\n",
    "    # Restore original column names\n",
    "    master.columns = masterColnames\n",
    "\n",
    "    # Save the final master file\n",
    "    os.makedirs(os.path.dirname(outputPath), exist_ok=True)  # Ensure directory exists\n",
    "    master.to_excel(outputPath, index=False)\n",
    "    print(f\"Master file updated and saved at {outputPath}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from openpyxl.utils import column_index_from_string\n",
    "\n",
    "def hyperlink():\n",
    "    folderPath = \"FOLDER\"\n",
    "    masterPath = \"Master datasheet.xlsx\"\n",
    "    outputPath = \"master/master.xlsx\"\n",
    "    targetCol = column_index_from_string('X')  # Column A for data\n",
    "    # Column B for filenames\n",
    "    ReferenceCol = column_index_from_string('AD')\n",
    "    fileCol = column_index_from_string('AC')\n",
    "    rowCount = 2\n",
    "\n",
    "    if os.path.exists(\"master\\\\master.xlsx\"):\n",
    "        target = load_workbook(\"master\\\\master.xlsx\")\n",
    "        targetSheet = target.active\n",
    "     # Add headers\n",
    "\n",
    "    for foldername in os.listdir(folderPath):\n",
    "        paths = os.path.join(folderPath, foldername)\n",
    "\n",
    "        for filename in os.listdir(paths):\n",
    "            try:\n",
    "                filePath = os.path.join(paths, filename)\n",
    "                Col_directions = pd.read_excel(masterPath, sheet_name=foldername)\n",
    "                source = load_workbook(filePath)\n",
    "                sourceSheet = source.active\n",
    "\n",
    "                sourceCol = column_index_from_string(Col_directions['Hfrom'][0])\n",
    "                RefCol = column_index_from_string(Col_directions['StkRf'][0])\n",
    "\n",
    "                for count, row in enumerate(sourceSheet.iter_rows(min_row=2), start=2):\n",
    "                    cell = row[sourceCol - 1]\n",
    "                    RefCell = row[RefCol-1]\n",
    "                    targetCell = targetSheet.cell(row=rowCount, column=targetCol)\n",
    "                    targetCell.value = cell.value\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    # Store filename in column B\n",
    "                    \n",
    "\n",
    "                    if cell.hyperlink:\n",
    "                        targetCell.hyperlink = cell.hyperlink\n",
    "                        targetCell.style = \"Hyperlink\"\n",
    "\n",
    "                        rowCount += 1\n",
    "\n",
    "                print(f\"Processed {filename} successfully\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    target.save(outputPath)\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def database_transfer():\n",
    "    conn = psycopg2.connect(\n",
    "        user=\"postgres\",\n",
    "        host=\"localhost\",\n",
    "        database=\"Master_Compilation\",\n",
    "        password=\"namanjain\",\n",
    "        port=5432,\n",
    "    )\n",
    "    db = conn.cursor()\n",
    "\n",
    "    # Create table if not exists\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS diamonds (\n",
    "        date DATE NOT NULL,\n",
    "        stock_ref TEXT NOT NULL,\n",
    "        company TEXT,\n",
    "        shape TEXT,\n",
    "        size NUMERIC(5,2),\n",
    "        colour TEXT,\n",
    "        clarity TEXT,\n",
    "        cut TEXT,\n",
    "        polish TEXT,\n",
    "        symmetry TEXT,\n",
    "        fluorescence TEXT,\n",
    "        ratio NUMERIC(6,3),\n",
    "        M1 NUMERIC(6,2),\n",
    "        M2 NUMERIC(6,2),\n",
    "        M3 NUMERIC(6,2),\n",
    "        depth NUMERIC(5,2),\n",
    "        table_percent NUMERIC(5,2),\n",
    "        rap_rate NUMERIC(10,2),\n",
    "        discount NUMERIC(5,2),\n",
    "        ppc NUMERIC(10,2),\n",
    "        total NUMERIC(10,2),\n",
    "        total_in_rs NUMERIC(10,2),\n",
    "        report_no TEXT,\n",
    "        cert_link TEXT,\n",
    "        video_link TEXT CHECK (video_link ~* '^https?://.+'),\n",
    "        bin_c TEXT,\n",
    "        bin_s TEXT,\n",
    "        comment TEXT,\n",
    "        comment_2 TEXT,\n",
    "        PRIMARY KEY (date, stock_ref)\n",
    "    );\n",
    "    \"\"\"\n",
    "    db.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Read Excel file\n",
    "    file = \"master/master.xlsx\"\n",
    "    wb = load_workbook(file, data_only=True)\n",
    "    ws = wb.active\n",
    "    df = pd.read_excel(file, engine=\"openpyxl\")\n",
    "\n",
    "    # Extract hyperlinks for the \"Video link\" column\n",
    "    video_links = []\n",
    "    for row in ws.iter_rows(\n",
    "        min_row=2, max_row=ws.max_row, min_col=df.columns.get_loc(\"Video link\") + 1, max_col=df.columns.get_loc(\"Video link\") + 1\n",
    "    ):\n",
    "        cell = row[0]\n",
    "        video_links.append(cell.hyperlink.target if cell.hyperlink else None)\n",
    "    \n",
    "    df[\"Video link\"] = video_links\n",
    "\n",
    "    # Convert numeric columns\n",
    "    numeric_columns = [\"Size\", \"Ratio\", \"M1\", \"M2\", \"M3\", \"Depth\", \"Table\", \"Rap rate\", \"Discount\", \"PPC\", \"Total\"]\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Insert data\n",
    "    insert_query = \"\"\"\n",
    "     insert into diamonds (Company,Shape,Size,ColourCclarity,Cut,Polish,Symmetry,\n",
    "            Flour,Ratio,M1,M2,M3,Depth,Tables,rap_rate,discount,ppc,total,total_in_rs,report_no,\n",
    "            stockref,cert_link,video_link,binC,binS,comment,comment2) Values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "    ON CONFLICT (date, stock_ref) DO NOTHING;\n",
    "    \"\"\"\n",
    "    print(list(df.columns))\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # print(row.values)\n",
    "        db.execute(insert_query, row)\n",
    "\n",
    "    conn.commit()\n",
    "    db.close()\n",
    "    conn.close()\n",
    "    print(\"Successfully added!\")\n",
    "\n",
    "database_transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def database_transfer():\n",
    "    conn = psycopg2.connect(\n",
    "        user= \"postgres\",\n",
    "        host= \"localhost\",\n",
    "        database= \"Master_Compilation\",\n",
    "        password=\"namanjain\",\n",
    "        port= 5432,\n",
    "    )\n",
    "    db = conn.cursor()\n",
    "    file = \"master\\master.xlsx\"\n",
    "    wb = load_workbook(file, data_only=True)\n",
    "    ws = wb.active\n",
    "    df = pd.read_excel(file,engine=\"openpyxl\")\n",
    "    columns = list(df.columns)\n",
    "    columns.append('Date')\n",
    "    vedio_links=[]\n",
    "    for row in ws.iter_rows(min_row=2,max_row=ws.max_row,min_col=df.columns.get_loc(\"Video link\")+1,max_col=df.columns.get_loc(\"Video link\")+1):\n",
    "        cell = row[0]\n",
    "        if cell.hyperlink:\n",
    "            vedio_links.append(cell.hyperlink.target)\n",
    "        else:\n",
    "            vedio_links.append(None)\n",
    "    df['Video link'] =  vedio_links\n",
    "    numeric_columns = [\"Size\", \"Ratio\", \"M1\", \"M2\", \"M3\", \"Depth\", \"Table\",\n",
    "                   \"Rap rate\", \"Discount\", \"PPC\", \"Total\"]\n",
    "    for col in numeric_columns:\n",
    "        df[col] = pd.to_numeric(df[col],errors='coerce')\n",
    "    display(df)\n",
    "    for index, row in df.iterrows():\n",
    "        db.execute(''' insert into diamonds (company,shape,size,colour,clarity,cut,polish,symmetry,\n",
    "                flour,ratio,m1,m2,m3,depth,tables,rap_rate,discount,ppc,total,total_in_rs,report_no,\n",
    "                stockref,cert_link,video_link,binC,binS,comment,comment2) Values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)''',\n",
    "                tuple(row))\n",
    "    conn.commit()\n",
    "    db.close()\n",
    "    conn.close()\n",
    "    print(\"successfully added\")\n",
    "    \n",
    "        \n",
    "database_transfer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_13848\\1775762361.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master = pd.concat([master, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SrNo', 'Location', 'Image', 'View360', 'LAB', 'DNA', 'Verify', 'Refno',\n",
      "       'Group', 'Shape', 'Carat', 'Threads', 'Rapaport', 'Price/CTS RS',\n",
      "       'TotalValue RS', 'OffPer', 'RapOff5Cts', 'BidType', 'BID(%)',\n",
      "       'BID Price', 'BIDValidTill', 'Color', 'CS', 'Clarity', 'Cut', 'Polish',\n",
      "       'Symm', 'Fl', 'FC', 'Measurement', 'TD%', 'Tab%', 'NOBGM', 'Grd%',\n",
      "       'CrAngPer', 'HA', 'Ratio', 'TI', 'BIS', 'BIC', 'TOI', 'EyeClean',\n",
      "       'Luster', 'CrownHeight', 'PavalionHeight', 'PavalionAngle', 'CU',\n",
      "       'OPPV', 'OPCR', 'OPTA', 'EFCR', 'EFPV', 'Natural-PV', 'Natural-CR',\n",
      "       'Natural-GRD', 'INatural-PV', 'INatural-CR', 'INatural-GRD', 'Graining',\n",
      "       'SurfaceGraining', 'GirdleDesc', 'GirdleMinMax%', 'GC', 'StarLength',\n",
      "       'LowerHalf', 'Cert. Type', 'Cert.No', 'Cert.Date', 'MultiGroup',\n",
      "       'Type2', 'FM', 'DOR Code', 'SealedCode', 'MineName', 'QC', 'INS',\n",
      "       'RoughOrigin', 'DOR No', 'LabControlNo', 'KeytoSymbols',\n",
      "       'ReportComments', 'RPComment', 'BuyerComment', 'Username'],\n",
      "      dtype='object')\n",
      "Index(['SrNo', 'StoneId', 'Shape', 'Color', 'Clarity', 'Carat', 'Rap', 'Disc%',\n",
      "       'PerCarat', 'Amount', 'Cut', 'Pol', 'Sym', 'Flo', 'Lab', 'ReportNo',\n",
      "       'Measurement', 'Depth%', 'Table%', 'Shade', 'NGS Comment', 'EyeClean',\n",
      "       'ct_into_rap', 'ct_into_rate', 'extra_field', 'Unnamed: 25',\n",
      "       'for solving per carat rate'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_13848\\1775762361.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  master = pd.concat([master, temp_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sr.No', 'Packet No', 'Status / Location', 'Cert', 'Shape', 'Carats',\n",
      "       'Price/Cts INR', 'Disc%', 'Tbl Incl', 'Measurement', 'L:W', 'Grd %',\n",
      "       'Incl Ptrn', 'Int Grn', 'Int Grn Typ', 'Tbl Opn', 'Crn Opn', 'Pav Opn',\n",
      "       'Grd Crn Opn', 'LP', 'Blk Incl', 'Table Black', 'Clarity', 'Color',\n",
      "       'Type2 Certi', 'Color Shade', 'Cut', 'Pol', 'Sym', 'Fluro', 'Fl. Col',\n",
      "       'FE', 'Luster', 'Naked Eye', 'Depth %', 'Table %', 'Polished',\n",
      "       'Front Hand', 'Back Hand', ' Tweezer', 'Light Video', 'Dark Video',\n",
      "       'Video with Details', 'MP4 Video', 'Plotting', 'Fluorescence',\n",
      "       'Journey', 'Consumer Video', 'ASET', 'Hearts & Arrows', 'Key To Symbol',\n",
      "       'Additional Comments', 'Disc Price', 'Disc Total', 'Total Amt'],\n",
      "      dtype='object')\n",
      "Master file updated and saved at master/master.xlsx.\n",
      "Processed DDPLStock-25-Feb-2025 17_49_28.xlsx successfully\n",
      "Processed Finestar.xlsx successfully\n",
      "Processed Jbbros rounds.xlsx successfully\n",
      "Processed Jodhani-Brothers-Stock-25022025-09-20-26.xlsx successfully\n",
      "Processed Narola.xlsx successfully\n",
      "Processed Sheetal.xlsx successfully\n",
      "Processed srk3.xlsx successfully\n",
      "Processed venus 3.xlsx successfully\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "t1= threading.Thread(target=masterfile)\n",
    "t2 = threading.Thread(target=hyperlink)\n",
    "\n",
    "t1.start()\n",
    "t1.join()\n",
    "t2.start()\n",
    "t2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
